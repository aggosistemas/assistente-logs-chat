# ==============================================================
# ü§ñ app/services/openai_client.py
# --------------------------------------------------------------
# Gera respostas t√©cnicas com base em logs reais do Firestore.
# Inclui sumariza√ß√£o local autom√°tica e filtro sem√¢ntico de contexto.
# ==============================================================

import os
from dotenv import load_dotenv
from openai import OpenAI
from app.utils.validation import is_prompt_valid
from app.services.firestore_context import obter_contexto_firestone
from app.utils.sanitize import sanitize_text

load_dotenv()

# ==============================================================
# üîë Inicializa√ß√£o do cliente OpenAI
# ==============================================================

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("‚ùå Vari√°vel de ambiente OPENAI_API_KEY n√£o encontrada.")

try:
    client = OpenAI(api_key=api_key)
    print("‚úÖ [OpenAI] Cliente inicializado com sucesso.")
except Exception as e:
    raise RuntimeError(f"‚ö†Ô∏è Falha ao inicializar o cliente OpenAI: {e}")

# ==============================================================
# üß© Fun√ß√£o auxiliar: sumariza√ß√£o local
# ==============================================================

def resumir_contexto_local(contexto: str, limite: int = 3000) -> str:
    """
    Reduz o tamanho do contexto antes de enviar √† API da OpenAI.
    Mant√©m apenas os trechos t√©cnicos mais relevantes.
    """
    if not contexto:
        return "Sem contexto t√©cnico relevante dispon√≠vel."

    linhas = contexto.split("\n")
    linhas_filtradas = []

    for linha in linhas:
        linha_lower = linha.lower()
        if any(p in linha_lower for p in ["erro", "falha", "exception", "timeout", "api", "warn", "sucesso", "mensagem"]):
            linhas_filtradas.append(linha)
        if len(linhas_filtradas) >= 50:
            break

    texto_final = "\n".join(linhas_filtradas) or contexto[:limite]
    texto_final = sanitize_text(texto_final)
    print(f"üß† [Resumo local] Contexto reduzido para {len(texto_final)} caracteres.")
    return texto_final


# ==============================================================
# üß† Fun√ß√£o principal: gerar resposta
# ==============================================================

def gerar_resposta(pergunta: str) -> str:
    """
    Gera resposta t√©cnica com base em logs do Firestore.
    - Valida o contexto da pergunta.
    - Busca contexto real dos logs.
    - Aplica resumo local antes de enviar √† OpenAI.
    """

    # üõ°Ô∏è 1. Valida√ß√£o sem√¢ntica
    if not is_prompt_valid(pergunta):
        return (
            "üö´ Sua pergunta parece fora do contexto t√©cnico. "
            "Por favor, pergunte algo relacionado a logs, falhas, "
            "monitoramento, sistemas corporativos ou incidentes."
        )

    if len(pergunta) > 1000:
        return "‚ö†Ô∏è A pergunta √© muito longa. Resuma o problema e tente novamente."

    # üîç 2. Buscar contexto t√©cnico real do Firestore
    try:
        contexto_logs = obter_contexto_firestone(pergunta)
    except Exception as e:
        print(f"‚ö†Ô∏è [Firestore] Erro ao obter contexto: {e}")
        contexto_logs = "N√£o foi poss√≠vel recuperar o contexto t√©cnico neste momento."

    # üß© 3. Reduzir o contexto localmente para otimizar custo e foco
    contexto_resumido = resumir_contexto_local(contexto_logs)

    # üß± 4. Montar o prompt t√©cnico
    prompt = f"""
    Voc√™ √© um assistente t√©cnico especializado em sustenta√ß√£o de sistemas corporativos.
    Use o contexto real de logs abaixo como base para responder √† pergunta.

    üîπ CONTEXTO FIRESTORE (resumido):
    {contexto_resumido}

    üîπ PERGUNTA:
    {pergunta}

    Responda com foco t√©cnico, descrevendo poss√≠veis causas, sintomas e recomenda√ß√µes.
    Se poss√≠vel, cite a origem dos logs (ex: [vida_nova_logs], [controle_auditoria_logs]).
    """

    # ü§ñ 5. Gera√ß√£o da resposta via OpenAI
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Voc√™ √© um assistente t√©cnico de sustenta√ß√£o de sistemas, "
                        "com experi√™ncia em observabilidade, logs, an√°lise de falhas e APIs corporativas. "
                        "Responda sempre com base nos dados do Firestore e evite generaliza√ß√µes."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.35,
            max_tokens=700,
        )

        resposta = response.choices[0].message.content.strip()
        print(f"‚úÖ [OpenAI] Resposta gerada com sucesso ({len(resposta)} caracteres).")
        return resposta

    except Exception as e:
        print(f"‚ùå [OpenAI] Erro ao gerar resposta: {e}")
        return (
            "‚ö†Ô∏è Ocorreu um erro ao gerar a resposta. "
            "Verifique os logs de execu√ß√£o para mais detalhes."
        )

# ==============================================================
# üß† Fun√ß√£o aprimorada: buscar contexto t√©cnico real no Firestore
#    com filtro sem√¢ntico baseado em embeddings
# ==============================================================

import math
from app.services.firestore_client import get_firestore_client
from app.utils.sanitize import sanitize_text
from firebase_admin import firestore

# --------------------------------------------------------------
# üîß Fun√ß√£o auxiliar: calcular similaridade coseno
# --------------------------------------------------------------
def cosine_similarity(vec1, vec2):
    """Calcula similaridade coseno entre dois vetores num√©ricos."""
    if not vec1 or not vec2 or len(vec1) != len(vec2):
        return 0.0
    dot = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0


# --------------------------------------------------------------
# üîç Fun√ß√£o principal
# --------------------------------------------------------------
def obter_contexto_firestone(pergunta: str, limite: int = 10) -> str:
    from app.services.openai_client import generate_embedding
    """
    Busca documentos no Firestore relacionados ao tema da pergunta.
    Utiliza embeddings para ranquear semanticamente os logs.
    Retorna um resumo textual consolidado para o modelo OpenAI.
    """
    db = get_firestore_client()
    pergunta_lower = pergunta.lower()

    # üîé Mapeamento expandido (mantido do seu c√≥digo atual)
    colecao_map = {
        "vida nova": "vida_nova_logs",
        "vida": "vida_nova_logs",
        "nova": "vida_nova_logs",
        "auditoria": "controle_auditoria_logs",
        "controle": "controle_auditoria_logs",
        "orcamento": "orcamento_contratacao_logs",
        "contratacao": "orcamento_contratacao_logs",
        "contrata√ß√£o": "orcamento_contratacao_logs",
        "viagem": "viagem_transmissao_logs",
        "transmissao": "viagem_transmissao_logs",
        "transmiss√£o": "viagem_transmissao_logs",
        "erro": "controle_auditoria_logs",
        "falha": "controle_auditoria_logs",
        "api": "orcamento_contratacao_logs",
        "sistema": "vida_nova_logs",
        "servi√ßo": "orcamento_contratacao_logs",
        "microservi√ßo": "orcamento_contratacao_logs",
        "plataforma": "viagem_transmissao_logs",
    }

    # üîç Identifica qual cole√ß√£o √© mais prov√°vel
    colecao = None
    for termo, nome_colecao in colecao_map.items():
        if termo in pergunta_lower:
            colecao = nome_colecao
            break

    # üîÅ fallback multi-cole√ß√£o
    if not colecao:
        print("‚ö†Ô∏è Nenhum termo espec√≠fico encontrado, aplicando fallback multi-cole√ß√£o.")
        colecoes = [
            "vida_nova_logs",
            "controle_auditoria_logs",
            "orcamento_contratacao_logs",
            "viagem_transmissao_logs",
        ]
    else:
        colecoes = [colecao]

    # ==============================================================
    # üß† Gera embedding da pergunta
    # ==============================================================
    pergunta_embedding = generate_embedding(pergunta)
    if not pergunta_embedding:
        print("‚ö†Ô∏è N√£o foi poss√≠vel gerar embedding da pergunta.")
        return "N√£o foi poss√≠vel gerar embedding da pergunta."

    # ==============================================================
    # üì• Leitura e ranqueamento sem√¢ntico dos documentos
    # ==============================================================
    candidatos = []
    for col in colecoes:
        try:
            print(f"üìÇ Buscando contexto em Firestore: cole√ß√£o '{col}'")
            docs = (
                db.collection(col)
                .order_by("timestamp", direction=firestore.Query.DESCENDING)
                .limit(limite * 3)  # l√™ mais registros para ranquear
                .stream()
            )

            for doc in docs:
                data = doc.to_dict()
                texto_log = " ".join([str(v) for v in data.values() if isinstance(v, str)])
                texto_log = sanitize_text(texto_log)
                if not texto_log:
                    continue

                # Gera embedding do log (modo leve)
                emb_log = generate_embedding(texto_log[:500])
                if not emb_log:
                    continue

                score = cosine_similarity(pergunta_embedding, emb_log)
                candidatos.append((score, col, texto_log))
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao ler cole√ß√£o {col}: {e}")

    if not candidatos:
        return "Nenhum log relevante foi encontrado nas cole√ß√µes dispon√≠veis."

    # üî¢ Ordena por relev√¢ncia
    candidatos.sort(key=lambda x: x[0], reverse=True)
    top = candidatos[:limite]

    # üîó Monta o contexto final consolidado
    contexto = [f"[{col}] {texto}" for _, col, texto in top]
    contexto_final = "\n".join(contexto)
    print(f"‚úÖ Contexto coletado e ranqueado: {len(top)} registros de {len(colecoes)} cole√ß√µes")
    return contexto_final[:6000]

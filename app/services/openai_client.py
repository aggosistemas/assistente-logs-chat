# ==============================================================
# ü§ñ app/services/openai_client.py
# --------------------------------------------------------------
# Gera respostas t√©cnicas, de sustenta√ß√£o, de engenharia ou gerenciais
# com base em logs reais do Firestore.
# Inclui sumariza√ß√£o local, filtro sem√¢ntico de contexto e
# adapta√ß√£o autom√°tica de linguagem conforme o perfil da pergunta.
# ==============================================================

import os
from dotenv import load_dotenv
from openai import OpenAI
from app.utils.validation import is_prompt_valid
from app.services.firestore_context import obter_contexto_firestone
from app.utils.sanitize import sanitize_text

load_dotenv()

# ==============================================================
# üîë Inicializa√ß√£o do cliente OpenAI
# ==============================================================

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("‚ùå Vari√°vel de ambiente OPENAI_API_KEY n√£o encontrada.")

try:
    client = OpenAI(api_key=api_key)
    print("‚úÖ [OpenAI] Cliente inicializado com sucesso.")
except Exception as e:
    raise RuntimeError(f"‚ö†Ô∏è Falha ao inicializar o cliente OpenAI: {e}")

# ==============================================================
# üßÆ Fun√ß√£o de gera√ß√£o de Embeddings
# ==============================================================

def generate_embedding(texto: str) -> list:
    """
    Gera o embedding sem√¢ntico de um texto (log, pergunta ou contexto).
    Usa o modelo `text-embedding-3-small` para custo otimizado.
    Retorna uma lista de floats representando o vetor sem√¢ntico.
    """
    if not texto or not isinstance(texto, str):
        print("‚ö†Ô∏è [OpenAI] Texto inv√°lido para gera√ß√£o de embedding.")
        return []

    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=texto
        )
        embedding = response.data[0].embedding
        print(f"‚úÖ [OpenAI] Embedding gerado ({len(embedding)} dimens√µes).")
        return embedding
    except Exception as e:
        print(f"‚ùå [OpenAI] Erro ao gerar embedding: {e}")
        return []

# ==============================================================
# üß© Fun√ß√£o auxiliar: sumariza√ß√£o local
# ==============================================================

def resumir_contexto_local(contexto: str, limite: int = 3000) -> str:
    """
    Reduz o tamanho do contexto antes de enviar √† API da OpenAI.
    Mant√©m apenas os trechos t√©cnicos mais relevantes.
    """
    if not contexto:
        return "Sem contexto t√©cnico relevante dispon√≠vel."

    linhas = contexto.split("\n")
    linhas_filtradas = []

    for linha in linhas:
        linha_lower = linha.lower()
        if any(p in linha_lower for p in [
            "erro", "falha", "exception", "timeout", "api", "warn", "sucesso", "mensagem"
        ]):
            linhas_filtradas.append(linha)
        if len(linhas_filtradas) >= 50:
            break

    texto_final = "\n".join(linhas_filtradas) or contexto[:limite]
    texto_final = sanitize_text(texto_final)
    print(f"üß† [Resumo local] Contexto reduzido para {len(texto_final)} caracteres.")
    return texto_final


# ==============================================================
# üß† Fun√ß√£o principal: gerar resposta com perfis autom√°ticos
# ==============================================================

def gerar_resposta(pergunta: str) -> str:
    """
    Gera resposta adaptada ao perfil do usu√°rio:
    - Gestor/Diretor ‚Üí vis√£o gerencial e estrat√©gica
    - Analista de Sustenta√ß√£o/SRE ‚Üí vis√£o operacional
    - Desenvolvedor ‚Üí vis√£o de engenharia de software
    - T√©cnico (default) ‚Üí vis√£o t√©cnica gen√©rica
    """

    # üõ°Ô∏è 1. Valida√ß√£o sem√¢ntica
    if not is_prompt_valid(pergunta):
        return (
            "üö´ Sua pergunta parece fora do contexto t√©cnico. "
            "Por favor, pergunte algo relacionado a logs, falhas, "
            "monitoramento, sistemas corporativos ou incidentes."
        )

    if len(pergunta) > 1000:
        return "‚ö†Ô∏è A pergunta √© muito longa. Resuma o problema e tente novamente."

    # üß≠ 2. Detec√ß√£o de perfil do usu√°rio (gerencial, sustenta√ß√£o, engenharia, t√©cnico)
    pergunta_lower = pergunta.lower()

    if any(p in pergunta_lower for p in [
        "gestor", "diretor", "gerente", "sa√∫de t√©cnica", "status",
        "resumo geral", "panorama", "vis√£o executiva", "indicadores"
    ]):
        estilo_usuario = "gerencial"
        estilo_instrucao = (
            "Adote uma linguagem gerencial e anal√≠tica, "
            "fornecendo uma vis√£o executiva da sa√∫de t√©cnica dos sistemas. "
            "Evite jarg√µes de c√≥digo e foque em indicadores, riscos, tend√™ncias "
            "e recomenda√ß√µes estrat√©gicas para decis√£o."
        )

    elif any(p in pergunta_lower for p in [
        "analista", "sustenta√ß√£o", "suporte", "infraestrutura", "t√©cnico", "sre"
    ]):
        estilo_usuario = "sustenta√ß√£o"
        estilo_instrucao = (
            "Adote uma linguagem t√©cnica operacional, "
            "focando em logs, sintomas, causas prov√°veis e etapas de mitiga√ß√£o. "
            "Forne√ßa instru√ß√µes pr√°ticas para diagn√≥stico e corre√ß√£o, "
            "sem se aprofundar em c√≥digo-fonte."
        )

    elif any(p in pergunta_lower for p in [
        "dev", "programador", "engenheiro", "desenvolvedor", "pleno", "s√™nior"
    ]):
        estilo_usuario = "engenharia"
        estilo_instrucao = (
            "Adote uma linguagem t√©cnica avan√ßada voltada a desenvolvedores, "
            "incluindo detalhes sobre classes, APIs, depend√™ncias, arquitetura e performance. "
            "Forne√ßa insights sobre padr√µes de projeto, refatora√ß√£o e boas pr√°ticas de c√≥digo."
        )

    else:
        estilo_usuario = "t√©cnico"
        estilo_instrucao = (
            "Adote uma linguagem t√©cnica e detalhada, "
            "analisando causas, sintomas, logs e poss√≠veis solu√ß√µes operacionais. "
            "Inclua recomenda√ß√µes pr√°ticas e diagn√≥sticos espec√≠ficos."
        )

    print(f"üß© Modo de resposta: {estilo_usuario.upper()}")

    # üîç 3. Buscar contexto t√©cnico real do Firestore
    try:
        contexto_logs = obter_contexto_firestone(pergunta)
    except Exception as e:
        print(f"‚ö†Ô∏è [Firestore] Erro ao obter contexto: {e}")
        contexto_logs = "N√£o foi poss√≠vel recuperar o contexto t√©cnico neste momento."

    # üß© 4. Reduzir o contexto localmente para otimizar custo e foco
    contexto_resumido = resumir_contexto_local(contexto_logs)

    # üß± 5. Montar o prompt adaptado
    prompt = f"""
    Voc√™ √© um assistente especializado em sustenta√ß√£o e engenharia de sistemas corporativos.
    {estilo_instrucao}

    üîπ CONTEXTO FIRESTORE (resumido):
    {contexto_resumido}

    üîπ PERGUNTA:
    {pergunta}

    Responda de acordo com o estilo acima.
    """

    # ü§ñ 6. Gera√ß√£o da resposta via OpenAI
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Voc√™ √© um assistente t√©cnico e gerencial de sustenta√ß√£o de sistemas. "
                        "Seu objetivo √© transformar logs e m√©tricas em insights claros e √∫teis. "
                        "Respeite o estilo pedido no prompt: "
                        "gerencial (executivo), sustenta√ß√£o (operacional), engenharia (dev) ou t√©cnico (padr√£o)."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.35,
            max_tokens=900,
        )

        resposta = response.choices[0].message.content.strip()
        print(f"‚úÖ [OpenAI] Resposta gerada com sucesso ({len(resposta)} caracteres).")
        return resposta

    except Exception as e:
        print(f"‚ùå [OpenAI] Erro ao gerar resposta: {e}")
        return (
            "‚ö†Ô∏è Ocorreu um erro ao gerar a resposta. "
            "Verifique os logs de execu√ß√£o para mais detalhes."
        )
